{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le jeu de données\n",
    "path = \"/home/onyxia/work/statapp_sujet26/\"\n",
    "file_name1=\"dataset_complet_avec_dummies_part_1.csv\"\n",
    "file_name2=\"dataset_complet_avec_dummies_part_2.csv\"\n",
    "df1= pd.read_csv(path+file_name1, sep=',',low_memory=False)\n",
    "df2= pd.read_csv(path+file_name2, sep=',',low_memory=False)\n",
    "df=pd.concat([df1,df2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifier la colonne 'grav' pour un problème de classification binaire\n",
    "df['grav'] = df['grav'].replace({1: 0, 2: 0, 3: 1, 4: 1})\n",
    "\n",
    "# Diviser le jeu de données en ensembles d'entraînement et de test\n",
    "y = df['grav']\n",
    "X = df.drop(columns=['grav'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un classifieur Random Forest\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Entraîner le modèle\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Extraire l'importance des features\n",
    "importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Calculer le pourcentage d'importance de chaque feature\n",
    "total_importance = np.sum(importances)\n",
    "percentage_importance = (importances / total_importance) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Feature  Importance (%)\n",
      "0                     vma        4.758959\n",
      "1                     nbv        5.538648\n",
      "2    region_Île-de-France        1.603772\n",
      "3                catr_3.0        1.794340\n",
      "4                 agg_2.0        1.647418\n",
      "..                    ...             ...\n",
      "104               int_8.0        1.067633\n",
      "105              surf_8.0        1.002124\n",
      "106             infra_7.0        1.194715\n",
      "107              surf_6.0        0.727741\n",
      "108              surf_4.0        4.483516\n",
      "\n",
      "[109 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Créer un DataFrame pour stocker les noms des features et leur pourcentage d'importance\n",
    "feature_names = X.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_df = pd.DataFrame({'Feature': feature_names[indices], 'Importance (%)': percentage_importance})\n",
    "\n",
    "print(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 350}\n"
     ]
    }
   ],
   "source": [
    "# Définir les hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300, 350],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Recherche par grille pour trouver les meilleurs hyperparamètres\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres\n",
    "print(\"Meilleurs hyperparamètres:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle avec les meilleurs hyperparamètres\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "best_rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = best_rf_classifier.predict(X_test)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Précision avec les meilleurs hyperparamètres:\", accuracy)\n",
    "\n",
    "\n",
    "# Afficher l'importance de chaque feature avec les meilleurs hyperparamètres\n",
    "importances = best_rf_classifier.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Importance des variables dans le modèle Random Forest (avec meilleurs hyperparamètres)\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), [X.columns[i] for i in indices], rotation=45)\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
